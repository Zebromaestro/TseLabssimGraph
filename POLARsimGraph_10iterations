# ==============================================================================
# SCRIPT OVERVIEW
#
# This script is designed for high-performance processing of scientific simulation
# data. Its primary function is to read a collection of text-based data files
# from a specified directory, each representing a snapshot in time of a simulation.
# It uses the Polars library to efficiently parse these files, extract metadata from
# headers, and load the main numerical data.
#
# Key operations include:
# 1. Parsing each file to create two DataFrames: one with the full dataset and
#    another with a summary of key maximum values (e.g., max temperature).
# 2. Aggregating the results from all files into two master DataFrames.
# 3. Calculating the area under the curve for various chemical species over time.
# 4. Generating and saving interactive HTML graphs using Plotly Express to
#    visualize the results, including an "S-Curve" of temperature vs. mass flow
#    rate and animated plots of species concentrations.
#
# The script leverages Polars' internal parallelization and efficient memory
# management, making it significantly faster than traditional pandas-based approaches
# for this type of I/O- and CPU-bound task.
# ==============================================================================

import numpy as np
import polars as pl
import plotly.express as px
import plotly.io as pio
import glob
import os
from natsort import os_sorted
from itertools import islice
import re
import io
import time


# ==============================================================================
# POLARS-BASED DATA PROCESSING FUNCTIONS
#
# The original pandas-based functions have been replaced with these more
# efficient versions using the Polars library. The manual parallelization
# with MPIRE is no longer needed as Polars handles parallel operations
# internally.
# ==============================================================================

def process_file_pl(file_path):
    """
    Processes a single data file using Polars.

    This function reads file metadata from the header, then loads the main
    data into a Polars DataFrame. It calculates summary statistics (like max
    temperature) efficiently using Polars' built-in expression API.

    Args:
        file_path (str): The path to the input data file.

    Returns:
        tuple[pl.DataFrame, pl.DataFrame]: A tuple containing two Polars DataFrames:
        - The first contains the full data from the file.
        - The second contains a one-row summary of key maximum values.
    """

    def parse_header_line(line):
        """
        Robustly parses a header line to extract a label and a floating point value.
        This regex looks for any text followed by whitespace and a number in
        scientific notation at the end of the line.
        """
        # Match pattern: (any characters, non-greedy) (whitespace) (a number, possibly scientific)
        match = re.match(r'(.*?)\s+([-\d.E+]+)$', line.strip())
        if match:
            # If a match is found, separate it into the text label and the numeric value string.
            label, value_str = match.groups()
            return label.strip(), float(value_str)
        else:
            # If the line doesn't fit the expected format, raise an error.
            raise ValueError(f"Could not parse header line format: '{line.strip()}'")

    # Read the entire file into memory at once for efficient parsing.
    with open(file_path, 'r') as f:
        lines = f.readlines()

    # --- Header Parsing ---
    # The first 14 lines are expected to be the header.
    header_lines = lines[:14]
    try:
        # Parse specific lines (12 and 13) for flowrate and time, which are key metadata.
        flowrate_str, flowrate = parse_header_line(header_lines[12])
        time_str, time_val = parse_header_line(header_lines[13])
    except (ValueError, IndexError) as e:
        # If header parsing fails, raise a more informative error.
        raise type(e)(f"Error parsing header in {os.path.basename(file_path)}: {e}")

    # Extract the filename for use as a metadata column.
    filename = os.path.basename(file_path)

    # --- Data Pre-processing ---
    # Polars' read_csv is very strict with delimiters. To handle variable whitespace
    # between columns, we pre-process the data lines into a clean, comma-separated format.

    # Extract header from the 15th line and replace any multi-space separators with a single comma.
    header = re.sub(r'\s+', ',', lines[14].strip())

    # Do the same for all subsequent data rows, ensuring they are clean and uniformly formatted.
    data_rows = [re.sub(r'\s+', ',', line.strip()) for line in lines[15:] if line.strip()]

    # Combine the cleaned header and data rows into a single string that mimics a perfect CSV file.
    # This string is then read from an in-memory buffer, avoiding disk I/O.
    csv_buffer = io.StringIO(header + '\n' + '\n'.join(data_rows))

    # Read the cleaned data using Polars, specifying the now-consistent comma separator.
    df = pl.read_csv(csv_buffer, separator=',')

    # Drop columns that are entirely null, which can result from trailing spaces in the original file.
    all_null_cols = [col for col in df.columns if df[col].is_null().all()]
    df = df.drop(all_null_cols)

    # --- Create the summary DataFrame ('sortmax') using Polars aggregations ---
    # Define a list of aggregation expressions to be run. Polars can execute these in a single, optimized pass.
    aggs = [
        # Find the maximum temperature and name the resulting column 'Tmax(K)'.
        pl.col('Temperature(K)').max().alias('Tmax(K)'),
        # Find the index of the max temperature, then get the 'Radius(cm)' value at that same index.
        pl.col('Radius(cm)').gather(pl.col('Temperature(K)').arg_max()).alias('R_Tmax(cm)')
    ]

    # Conditionally add aggregations for optional species columns if they exist in the DataFrame.
    for species in ['CH', 'CHA', 'OH', 'OHA']:
        if species in df.columns:
            # For each species, find the radius at which its value is maximum.
            aggs.append(
                pl.col('Radius(cm)').gather(pl.col(species).arg_max()).alias(f'R_{species}max(cm)')
            )

    # Execute all defined aggregations in a single `select` statement for efficiency.
    # Then, add the metadata (time, flowrate, filename) as new columns.
    sort_df = df.select(aggs).with_columns(
        pl.lit(time_val).alias(time_str),
        pl.lit(flowrate).alias(flowrate_str),
        pl.lit(filename).alias('FileName')
    )

    # --- Create the main data DataFrame ---
    # Add the time and filename metadata to every row of the full dataset.
    data_df = df.with_columns(
        pl.lit(time_val).alias(time_str),
        pl.lit(filename).alias('FileName')
    )

    # Reorder columns to have metadata first for better readability.
    final_order = [time_str, 'FileName'] + [col for col in df.columns]
    data_df = data_df.select(final_order)

    return data_df, sort_df


def simParse_pl(directory):
    """
    Parses all simulation files in a directory using Polars.

    This function finds all 'DATA*' files, processes them individually
    using `process_file_pl`, and then concatenates the results into two
    main DataFrames. Polars' `concat` is highly efficient.

    Args:
        directory (str): The directory containing the data files.

    Returns:
        tuple[pl.DataFrame, pl.DataFrame]: A tuple containing the combined
        data and the combined summary statistics as Polars DataFrames.
    """
    # Find all files matching the 'DATA*' pattern and sort them naturally (e.g., DATA1, DATA2, ... DATA10).
    filelist = os_sorted(glob.glob(os.path.join(directory, "DATA*")))
    if not filelist:
        print(f"Warning: No 'DATA*' files found in directory: {directory}")
        return pl.DataFrame(), pl.DataFrame()

    # Initialize lists to hold the DataFrames from each file.
    data_list = []
    sortmax_list = []

    print(f'Starting Polars processing for {len(filelist)} files...')

    # Loop through each file path and process it.
    for file_path in filelist:
        try:
            # Process a single file to get its data and summary DataFrames.
            data_df, sort_df = process_file_pl(file_path)
            data_list.append(data_df)
            sortmax_list.append(sort_df)
        except Exception as e:
            # If an error occurs (e.g., malformed file), print a message and skip the file.
            print(f"Skipping file {os.path.basename(file_path)} due to error: {e}")

    # If no files were successfully processed, return empty DataFrames.
    if not data_list:
        print("No files were successfully processed.")
        return pl.DataFrame(), pl.DataFrame()

    print('Concatenating results into final DataFrames...')
    # Concatenate all individual DataFrames into two large ones.
    # 'how=diagonal' efficiently stacks DataFrames, filling missing columns with nulls.
    data = pl.concat(data_list, how='diagonal')
    sortmax = pl.concat(sortmax_list, how='diagonal')

    print('Finished DataFrame processing with Polars.')
    return data, sortmax


def calculate_area_under_curve_pl(data_df, time_column='Time(s)'):
    """
    Calculates the area under the curve for each chemical species vs. radius,
    grouped by time, using Polars and NumPy.

    Args:
        data_df (pl.DataFrame): The input DataFrame with time, radius, and species data.
        time_column (str): The name of the time column.

    Returns:
        pl.DataFrame: A pivoted DataFrame with species as rows and time-based
        area-under-curve calculations as columns.
    """
    print("Calculating area under the curve...")
    if data_df.is_empty() or 'Radius(cm)' not in data_df.columns:
        print("Input DataFrame is empty or missing 'Radius(cm)' column. Cannot calculate AUC.")
        return pl.DataFrame()

    # Dynamically identify columns that represent chemical species or other elements to integrate.
    # These are numeric columns that are not part of the standard metadata.
    id_vars = {time_column, 'Radius(cm)', 'FileName', 'Temperature(K)', 'Mass flow rate(g/s)'}
    element_cols = [
        col for col in data_df.columns
        if col not in id_vars and data_df[col].dtype in pl.NUMERIC_DTYPES
    ]

    if not element_cols:
        print("No species columns found to calculate area under curve.")
        return pl.DataFrame()

    # This helper function applies the trapezoidal rule from NumPy to a Polars group.
    def trapz_integral(series_struct):
        # Extract the species data (y-axis) and radius data (x-axis) from the struct.
        y = series_struct.struct.field(element)
        x = series_struct.struct.field('Radius(cm)')
        # Sort values by radius (x-axis) before integration, which is required by np.trapz.
        sorted_indices = x.arg_sort()
        # Perform the integration on the sorted NumPy arrays.
        return np.trapz(y.gather(sorted_indices).to_numpy(), x.gather(sorted_indices).to_numpy())

    # Build a list of aggregation expressions, one for each species column.
    auc_aggs = []
    for element in element_cols:
        # For each element, create an expression that packs its column and the radius column
        # into a temporary struct, applies the integration function, and aliases the result.
        auc_aggs.append(
            pl.struct([element, 'Radius(cm)'])
            .apply(trapz_integral)
            .alias(f"AUC_{element}")
        )

    # Group the data by each time step and apply all the area-under-curve aggregations.
    grouped_data = data_df.group_by(time_column).agg(auc_aggs)

    # Reshape the data from a wide format (one column per AUC) to a long format,
    # and then pivot it to have species as rows and time steps as columns. This is a
    # common format for reporting results.
    return grouped_data.melt(id_vars=[time_column]).pivot(
        values="value", index="variable", columns=time_column
    )


def graphScurve_pl(file_path, df):
    """Generates and saves a scatter plot for the S-Curve."""
    if df.is_empty():
        print("S-Curve DataFrame is empty, skipping graph generation.")
        return
    print("Generating S-Curve graph...")
    # Create a scatter plot of max temperature vs. mass flow rate.
    fig = px.scatter(
        df,
        x='Mass flow rate(g/s)',
        y='Tmax(K)',
        hover_data=['FileName'],  # Show the source filename on hover.
        template='plotly_dark',  # Use a dark theme for the plot.
        title='S-Curve: Max Temperature vs. Mass Flow Rate'
    )
    # Define the output path for the HTML file.
    output_path = os.path.join(file_path, 'a_scurve.html')
    # Write the plot to an HTML file. full_html=False makes it a self-contained div.
    with open(output_path, 'w') as f:
        f.write(pio.to_html(fig, full_html=False, auto_play=False))
    print(f"S-Curve graph saved to {output_path}")


def simGraph_pl(file_path, sm_df, data_df):
    """Generates and saves summary and animated graphs."""
    if sm_df.is_empty() or data_df.is_empty():
        print("Input DataFrames are empty, skipping simulation graph generation.")
        return

    print("Generating simulation summary and animated graphs...")
    # --- Graph 1: Summary Plot ---
    # Identify columns in the summary DataFrame that contain 'max' for plotting.
    y_cols = [c for c in sm_df.columns if 'max' in c.lower() and c != 'FileName']
    # Create a scatter plot of these maximum values over time.
    fig1 = px.scatter(
        sm_df,
        x='Time(s)',
        y=y_cols,
        hover_data=['FileName'],
        title='Sorted Maximums Over Time',
        template='plotly_dark'
    )

    # --- Graph 2: Animated Plot ---
    # Define which columns to keep for the animated plot.
    colKeepy = ['Temperature(K)', 'C2H4', 'O2', 'N2', 'H2O', 'CO2', 'CH', 'CHA', 'OH', 'OHA']
    # Filter this list to only include columns that actually exist in the data.
    value_vars = [c for c in colKeepy if c in data_df.columns]
    # Define the ID variables for the reshaping operation.
    id_vars = [c for c in ['Radius(cm)', 'Time(s)', 'FileName'] if c in data_df.columns]

    # Reshape the DataFrame from a "wide" format to a "long" format.
    # This is necessary for Plotly Express to create an animated plot where each species is a color.
    df_long = data_df.unpivot(
        index=id_vars,
        on=value_vars,
        variable_name='Species',  # The new column for the species names.
        value_name='Value'  # The new column for their corresponding values.
    )

    # Create the animated scatter plot.
    fig2 = px.scatter(
        df_long,
        x='Radius(cm)',
        y='Value',
        color='Species',  # Each species gets a unique color.
        animation_frame='Time(s)',  # The 'Time(s)' column drives the animation.
        title='Animated Species Profile vs. Radius',
        template='plotly_dark'
    )

    # --- Save Both Graphs to a Single HTML File ---
    output_path = os.path.join(file_path, 'a_graph.html')
    with open(output_path, 'w') as f:
        f.write("<h1>Sorted Maximums Plot</h1>")
        f.write(pio.to_html(fig1, full_html=False, auto_play=False))
        f.write("<h1>Animated Species Plot</h1>")
        f.write(pio.to_html(fig2, full_html=False, auto_play=False))
    print(f"Simulation graphs saved to {output_path}")


if __name__ == "__main__":
    # This block runs when the script is executed directly.

    num_iterations = 10
    all_durations = []

    # The directory where the 'DATA*' files are located.
    # IMPORTANT: Update this path to your specific directory.
    target_dir = "/Users/ryanmattana/Desktop/SPAM/sample data"
    print(f'Directory Input: {target_dir}')

    # --- Loop for Performance Measurement ---
    for i in range(num_iterations):
        print(f"\n----- Starting Iteration {i + 1}/{num_iterations} -----")
        # Start a timer for the current iteration.
        start_time = time.monotonic()

        # --- Main processing ---
        # Call the main parsing function to process all files in the directory.
        dataDf_pl, sortmax_pl = simParse_pl(target_dir)

        # --- Save results ---
        # Proceed only if the processing returned non-empty DataFrames.
        if not dataDf_pl.is_empty() and not sortmax_pl.is_empty():
            # Define file paths for the output pickle files.
            data_pickle_path = os.path.join(target_dir, 'dataDf.pkl')
            sortmax_pickle_path = os.path.join(target_dir, 'sortmax.pkl')

            # Convert the Polars DataFrame to a pandas DataFrame before saving to a pickle file.
            # This is for compatibility if other tools in a workflow expect pandas objects.
            print(f"Saving data to {data_pickle_path}")
            dataDf_pl.to_pandas().to_pickle(data_pickle_path)

            print(f"Saving summary data to {sortmax_pickle_path}")
            sortmax_pl.to_pandas().to_pickle(sortmax_pickle_path)

            # --- Generate visualizations ---
            # Call the functions to create and save the HTML graphs.
            graphScurve_pl(target_dir, sortmax_pl)
            simGraph_pl(target_dir, sortmax_pl, dataDf_pl)

        else:
            print("Processing finished with no data. Exiting.")

        # Stop the timer and record the duration for this iteration.
        end_time = time.monotonic()
        duration = end_time - start_time
        all_durations.append(duration)
        print(f"----- Iteration {i + 1} finished in {duration:.3f} seconds -----")

    # --- Calculate and Display Average Time ---
    if all_durations:
        average_time = sum(all_durations) / len(all_durations)
        print("\n================= PERFORMANCE SUMMARY =================")
        print(f"All run times (seconds): {[round(d, 3) for d in all_durations]}")
        print(f"Average execution time over {num_iterations} iterations: {average_time:.3f} seconds")
        print("=====================================================")
    else:
        print("\nNo iterations were successfully completed.")

